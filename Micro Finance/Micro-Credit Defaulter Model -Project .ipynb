{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7dea80a",
   "metadata": {},
   "source": [
    "# Micro-Credit Defaulter Model -Project "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138510ac",
   "metadata": {},
   "source": [
    "# Importing the Datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a802c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4078e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"microfinance Data file.csv\") #reading the data file\n",
    "df #diplaying the dataset with first 5 and last 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4224dc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape #The Total number of data (same as df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a7a3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns #to see the columns names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8041f7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head() #to see 5 first row of dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c41898",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35df7d3f",
   "metadata": {},
   "source": [
    "# Checking for NULL values if any in the data frame\n",
    "\n",
    "np.nan, None, NaN and others.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633ec202",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5562bda6",
   "metadata": {},
   "source": [
    "We dont see any null values but we have unnamed column which is just an index , so we will drop it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8e10e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CAN ALSO USE\n",
    "\n",
    "print (df.info()) #to check for null Values "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29b8d59",
   "metadata": {},
   "source": [
    "We have rechecked again and found the same null values as we saw and we will have to remove them to proceed further"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b07622a",
   "metadata": {},
   "source": [
    "# Treating the NULL values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b07727e",
   "metadata": {},
   "source": [
    "### Understanding the columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b49a8f8",
   "metadata": {},
   "source": [
    "label\t Flag indicating whether the user paid back the credit amount within 5 days of issuing the loan{1:success, 0:failure}\n",
    "msisdn\t        mobile number of user\n",
    "aon\t            age on cellular network in days\n",
    "daily_decr30\tDaily amount spent from main account, averaged over last 30 days (in Indonesian Rupiah)\n",
    "daily_decr90\tDaily amount spent from main account, averaged over last 90 days (in Indonesian Rupiah)\n",
    "rental30\t    Average main account balance over last 30 days\n",
    "rental90\t    Average main account balance over last 90 days\n",
    "last_rech_date_ma\tNumber of days till last recharge of main account\n",
    "last_rech_date_da\tNumber of days till last recharge of data account\n",
    "last_rech_amt_ma\tAmount of last recharge of main account (in Indonesian Rupiah)\n",
    "cnt_ma_rech30\tNumber of times main account got recharged in last 30 days\n",
    "fr_ma_rech30\tFrequency of main account recharged in last 30 days\n",
    "sumamnt_ma_rech30\tTotal amount of recharge in main account over last 30 days (in Indonesian Rupiah)\n",
    "medianamnt_ma_rech30\tMedian of amount of recharges done in main account over last 30 days at user level (in Indonesian Rupiah)\n",
    "medianmarechprebal30\tMedian of main account balance just before recharge in last 30 days at user level (in Indonesian Rupiah)\n",
    "cnt_ma_rech90\tNumber of times main account got recharged in last 90 days\n",
    "fr_ma_rech90\tFrequency of main account recharged in last 90 days\n",
    "sumamnt_ma_rech90\tTotal amount of recharge in main account over last 90 days (in Indonasian Rupiah)\n",
    "medianamnt_ma_rech90\tMedian of amount of recharges done in main account over last 90 days at user level (in Indonasian Rupiah)\n",
    "medianmarechprebal90\tMedian of main account balance just before recharge in last 90 days at user level (in Indonasian Rupiah)\n",
    "cnt_da_rech30\tNumber of times data account got recharged in last 30 days\n",
    "fr_da_rech30\tFrequency of data account recharged in last 30 days\n",
    "cnt_da_rech90\tNumber of times data account got recharged in last 90 days\n",
    "fr_da_rech90\tFrequency of data account recharged in last 90 days\n",
    "cnt_loans30\tNumber of loans taken by user in last 30 days\n",
    "amnt_loans30\tTotal amount of loans taken by user in last 30 days\n",
    "maxamnt_loans30\tmaximum amount of loan taken by the user in last 30 days\n",
    "medianamnt_loans30\tMedian of amounts of loan taken by the user in last 30 days\n",
    "cnt_loans90\tNumber of loans taken by user in last 90 days\n",
    "amnt_loans90\tTotal amount of loans taken by user in last 90 days\n",
    "maxamnt_loans90\tmaximum amount of loan taken by the user in last 90 days\n",
    "medianamnt_loans90\tMedian of amounts of loan taken by the user in last 90 days\n",
    "payback30\tAverage payback time in days over last 30 days\n",
    "payback90\tAverage payback time in days over last 90 days\n",
    "pcircle\ttelecom circle\n",
    "pdate\tdate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434d0f5a",
   "metadata": {},
   "source": [
    "## Treating the categorical column with mode of the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf035e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(columns = ['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9e83a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6678aeeb",
   "metadata": {},
   "source": [
    "We have treated the null values in the cuisines column and we can see there are nono on the heatmap as well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca942d5f",
   "metadata": {},
   "source": [
    "# Checking the unique values in the columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1278761",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a76c75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['msisdn'].nunique() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6766699e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['aon'].nunique() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e06a60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['daily_decr30'].nunique() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067b352a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['daily_decr90'].nunique() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6293c38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rental30'].nunique() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78459c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rental90'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4714f05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['last_rech_date_ma'].nunique() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000cae66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['last_rech_date_da'].nunique() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc09255",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['last_rech_amt_ma'].nunique()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fae455",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cnt_ma_rech30'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ab8a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['fr_ma_rech30'].nunique()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e5ca6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sumamnt_ma_rech30'].nunique()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cddf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sumamnt_ma_rech90'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b50cba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['fr_ma_rech90'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f146caf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pcircle'].unique() #this is one of the label we will work on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5259e95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pdate'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d344ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the unique values in each column\n",
    "\n",
    "for col in df:\n",
    "    print(df[col].nunique(),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba713b19",
   "metadata": {},
   "source": [
    "# Dropping the columns which will not be of use in model building like address and duplicate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e46a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['pcircle'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffc299a",
   "metadata": {},
   "source": [
    "# Checking for Duplicate Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154ac0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b40d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecaa6d9",
   "metadata": {},
   "source": [
    "We see there are no duplicate values as well , moving on to next wtep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92129c4",
   "metadata": {},
   "source": [
    "# Check the datatypes of the columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778b49f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fdb34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d848102",
   "metadata": {},
   "source": [
    "Observations:-\n",
    "\n",
    "    - We see that the aon, daily_decr30, daily_decr90,rental30,rental90,last_rech_date_ma and most of the other columns have extreme values , outliers causing the Std to be higher than the mean whichis not good need to treat them for if  many columns has outliers and the range is too extreme, the min says 0 which is not possible  which is again wrong, we need to treat this  \n",
    "    - we see that the label is classification and series on 0 and 1 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785a06b4",
   "metadata": {},
   "source": [
    "# Observations made in individual columns cells above . Overall there is a huge variation in the type of data , but the columns are all having pretty straightforward contents as we can make out what each means except the data which is object which we can treat\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ef9f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to see if any of the values in Target  is white spaces\n",
    "\n",
    "df.loc[df['label'] == \" \"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62812c10",
   "metadata": {},
   "source": [
    "## As we see that  the targets have no whitespaces we can move ahead"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e241c86",
   "metadata": {},
   "source": [
    "# Treating date column as it is object data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d969b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Day\"] = pd.to_datetime(df.pdate, format=\"%Y/%m/%d\").dt.day\n",
    "df[\"Month\"] = pd.to_datetime(df.pdate, format=\"%Y/%m/%d\").dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35585a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will not get year as all the year is same 2016 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b157cb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60abf65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(columns=['pdate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489ee643",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9588787a",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bcad99",
   "metadata": {},
   "source": [
    "# Visualization of the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b38ed50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "df.hist(figsize=(20,20))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0018893",
   "metadata": {},
   "source": [
    "To understand properly we need to review each feature individually but this graph is just for showing the trend with histplot only numeric features shown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f1be0a",
   "metadata": {},
   "source": [
    "# Splitting the columns with categorical and numerica data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7f3cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are defining numerical & categorical columns\n",
    "numeric_features = [feature for feature in df.columns if df[feature].dtype != 'O']\n",
    "categorical_features = [feature for feature in df.columns if df[feature].dtype == 'O']\n",
    "\n",
    "# print columns\n",
    "print('We have {} numerical features : {}'.format(len(numeric_features), numeric_features))\n",
    "print('\\nWe have {} categorical features : {}'.format(len(categorical_features), categorical_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef37cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_visualization_continuous=df[['aon', 'daily_decr30', 'daily_decr90', 'rental30', 'rental90', 'last_rech_date_ma', 'last_rech_date_da', 'last_rech_amt_ma', 'cnt_ma_rech30', 'fr_ma_rech30', 'sumamnt_ma_rech30', 'medianamnt_ma_rech30', 'medianmarechprebal30', 'cnt_ma_rech90', 'fr_ma_rech90', 'sumamnt_ma_rech90', 'medianamnt_ma_rech90', 'medianmarechprebal90', 'cnt_da_rech30', 'fr_da_rech30', 'cnt_da_rech90', 'fr_da_rech90', 'cnt_loans30', 'amnt_loans30', 'maxamnt_loans30', 'medianamnt_loans30', 'cnt_loans90', 'amnt_loans90', 'maxamnt_loans90', 'medianamnt_loans90', 'payback30', 'payback90', 'Day', 'Month']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886db588",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_visualization_nominal=df[['msisdn']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c92468",
   "metadata": {},
   "source": [
    "# Visualization of the distribution of the continuous value of the float and int columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6b03c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894f2ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets see how the data is distributed for every column\n",
    "\n",
    "plt.figure(figsize =(10,20), facecolor = 'white')\n",
    "plotnumber = 1\n",
    "\n",
    "for column in df_visualization_continuous:\n",
    "    if plotnumber <=35:\n",
    "        ax = plt.subplot(9,4,plotnumber)\n",
    "        sns.distplot(df_visualization_continuous[column])\n",
    "        plt.xlabel(column,fontsize = 8)\n",
    "        \n",
    "    plotnumber +=1\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cc218a",
   "metadata": {},
   "source": [
    "# Observations :-\n",
    "    \n",
    "   - We see that most of the columns are right skewed andhave outliers and extreme ranges as seen in df. describe chart as well , we need to treat them \n",
    "   - day and month are categorical in nature \n",
    "   - we seee some imbalance in the label where the 1is much mor than o so we have imbalanced dataset as well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef64e28",
   "metadata": {},
   "source": [
    "## Visualizing the Target Variable for classification only to check for imbalanced Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd1de1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.countplot(x='label',data = df)\n",
    "print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedaed69",
   "metadata": {},
   "outputs": [],
   "source": [
    "range_names=df[\"label\"].value_counts().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6304ec39",
   "metadata": {},
   "outputs": [],
   "source": [
    "range_val=df[\"label\"].value_counts().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11976284",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pie Chart for the target Price range\n",
    "plt.pie(range_val,labels=range_names,autopct='%1.2f%%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eed97a3",
   "metadata": {},
   "source": [
    "Observations :-\n",
    "\n",
    "We see a huge imbalance in the label column where the applicants whose gonna pay is is 87.52% and the ones whose isnt is 12.48% so we need to balance this otherwise the model will be biased and we cant have that  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d294785",
   "metadata": {},
   "source": [
    "## Lets graph the data for columns individually so we can make clear findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27f6e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_visualization_continuous.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f28e7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.distplot(df_visualization_continuous['aon'],kde=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb72200",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(x=df_visualization_continuous['aon'], ec = \"gold\", color='g', kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab72d869",
   "metadata": {},
   "source": [
    "We see that the data is right skewed and we many outliers in the dataset ,  , need to treat to get a better look at the data but as this as the model will not work well otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d326eeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.subplots(figsize=(12,4))\n",
    "sns.countplot(x=\"aon\", hue=\"label\", data=df, palette=\"colorblind\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e66054",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "plot_df=df.sort_values(by=\"label\")\n",
    "fig=px.histogram(plot_df, x='aon', color='label', \n",
    "                 opacity=0.8, histnorm='density', barmode='overlay', marginal='box',\n",
    "                 color_discrete_map={'Yes': '#B9C0C9','No': '#CDBBA7'})\n",
    "fig.update_layout(title_text='Distribution of aon by label Status',\n",
    "                  xaxis_title='aon, $', yaxis_title='Density',font_color='#28221D',\n",
    "                  paper_bgcolor='#F4F2F0', plot_bgcolor='#F4F2F0', legend_traceorder='reversed')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1ecb44",
   "metadata": {},
   "source": [
    "We see that we are not able to make any analysis ast he data is skwed to the right greatly and we are not able to make any observations without treating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab576be9",
   "metadata": {},
   "source": [
    "# As the rest of the columns face the same issue where the data is right skewed we wll only visualize the columsn with normal or atleast a spread out distribution as we are not able to make out anything with the skwed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8585e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['Day'],kde=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41ffe3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.countplot(x='Day',data = df)\n",
    "print(df['Day'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46ba873",
   "metadata": {},
   "source": [
    "We see we have equal transactions in all the days of the month , we see some dips in the last days but overall equal distribution of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6e8e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.subplots(figsize=(12,4))\n",
    "sns.countplot(x=\"Day\", hue=\"label\", data=df, palette=\"colorblind\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965d7425",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "plot_df=df.sort_values(by=\"label\")\n",
    "fig=px.histogram(plot_df, x='Day', color='label', \n",
    "                 opacity=0.8, histnorm='density', barmode='overlay', marginal='box',\n",
    "                 color_discrete_map={'Yes': '#B9C0C9','No': '#CDBBA7'})\n",
    "fig.update_layout(title_text='Distribution of Day by label Status',\n",
    "                  xaxis_title='Day, $', yaxis_title='Density',font_color='#28221D',\n",
    "                  paper_bgcolor='#F4F2F0', plot_bgcolor='#F4F2F0', legend_traceorder='reversed')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66144da",
   "metadata": {},
   "source": [
    "We see equal distribution of 0 and 1 in the columns and we cant say which dates have higher transactions with 0 as most are spread out , we see some dips in the end of the month "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa136b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['Month'],kde=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d851799",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.countplot(x='Month',data = df)\n",
    "print(df['Month'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0f62fa",
   "metadata": {},
   "source": [
    "We see that the most of the transactions are conducted in 6th month and 7th month and the least among is 8th month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2326ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.subplots(figsize=(12,4))\n",
    "sns.countplot(x=\"Month\", hue=\"label\", data=df, palette=\"colorblind\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51646694",
   "metadata": {},
   "outputs": [],
   "source": [
    "yes_group = df[df[\"label\"]== 0]\n",
    "no_group = df[df[\"label\"]!= 0]\n",
    "\n",
    "fig=plt.figure(figsize=(9,9))\n",
    "plt.style.use('seaborn-colorblind')\n",
    "fig.add_subplot(2,2,1)\n",
    "yes_group[\"Month\"].value_counts().plot(kind=\"pie\",  subplots=True,autopct='%1.1f%%', startangle=180)\n",
    "\n",
    "plt.title(' Distribution of No Month('+str(len(yes_group))+')');\n",
    "\n",
    "fig.add_subplot(2,2,2)\n",
    "no_group[\"Month\"].value_counts().plot(kind=\"pie\",  subplots=True,autopct='%1.1f%%', startangle=180)\n",
    "\n",
    "plt.title(' Distribution of Yes Month ('+str(len(no_group))+')');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df16c3b",
   "metadata": {},
   "source": [
    "We see most of the 0 are in the 6th and 7th month and null 0 are in the 8th month , but as per data 8th month has the least data so we are not sure if we can state 8 has none  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796c3419",
   "metadata": {},
   "source": [
    "# Treating the Skewness in the dta before other Visualizations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c58b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09256a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cd3287",
   "metadata": {},
   "outputs": [],
   "source": [
    "features= ['aon', 'daily_decr30', 'daily_decr90', 'rental30','rental90', 'last_rech_date_ma', 'last_rech_date_da','last_rech_amt_ma', 'cnt_ma_rech30', 'fr_ma_rech30','sumamnt_ma_rech30', 'medianamnt_ma_rech30', 'medianmarechprebal30','cnt_ma_rech90', 'fr_ma_rech90', 'sumamnt_ma_rech90','medianamnt_ma_rech90', 'medianmarechprebal90', 'cnt_da_rech30','fr_da_rech30', 'cnt_da_rech90', 'fr_da_rech90', 'cnt_loans30','amnt_loans30', 'maxamnt_loans30', 'medianamnt_loans30', 'cnt_loans90','amnt_loans90', 'maxamnt_loans90', 'medianamnt_loans90', 'payback30','payback90']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee59e00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "scaler =PowerTransformer(method = 'yeo-johnson')\n",
    "'''\n",
    "\n",
    "parameters:\n",
    "method ='box-cox' or 'yeo-johnson'\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec727b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_visualization_continuous[features] = scaler.fit_transform(df_visualization_continuous[features].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb375cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_visualization_continuous[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834a7cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_visualization_continuous.skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cef4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_visualization_continuous.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23da3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets see how the data is distributed for every column\n",
    "\n",
    "plt.figure(figsize=(20,25), facecolor='white')\n",
    "plotnumber= 1\n",
    "\n",
    "for column in df_visualization_continuous[features]:\n",
    "    if plotnumber <= 32:\n",
    "        ax = plt.subplot(8,4,plotnumber)\n",
    "        sns.distplot(df_visualization_continuous[column])\n",
    "        plt.xlabel(column,fontsize=8)\n",
    "        \n",
    "    plotnumber+=1\n",
    "plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82907648",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df.drop(columns=['Day', 'Month', 'aon', 'daily_decr30', 'daily_decr90', 'rental30','rental90', 'last_rech_date_ma', 'last_rech_date_da','last_rech_amt_ma', 'cnt_ma_rech30', 'fr_ma_rech30','sumamnt_ma_rech30', 'medianamnt_ma_rech30', 'medianmarechprebal30','cnt_ma_rech90', 'fr_ma_rech90', 'sumamnt_ma_rech90','medianamnt_ma_rech90', 'medianmarechprebal90', 'cnt_da_rech30','fr_da_rech30', 'cnt_da_rech90', 'fr_da_rech90', 'cnt_loans30','amnt_loans30', 'maxamnt_loans30', 'medianamnt_loans30', 'cnt_loans90','amnt_loans90', 'maxamnt_loans90', 'medianamnt_loans90', 'payback30','payback90'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518eae8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab47cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd .concat([df,df_visualization_continuous],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b67eb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd885db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e56816",
   "metadata": {},
   "source": [
    "# we are first calculating the skew value and some of the column skew value are far from zero almost 13 so its diificult to make a good model with this.\n",
    "\n",
    "- The best skew value for normally distributes is very close to zero, so we are using “log1p” method to make the skew value near to zero\n",
    "- In the last cell I am again checking the skewness value and there is difference between the first skewness value and second, now the skewness value of each column is near to zero.\n",
    "\n",
    "- Making the skewness value near to zero will help to get better score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f481cc65",
   "metadata": {},
   "source": [
    "### MULTIVARIATE ANALYSIS -WITH PAIRPLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50bd0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86344b04",
   "metadata": {},
   "source": [
    "## We see a number of observation in the pairplot , but the relationship is very hard to pinpoint so we will need to plot different relationship plots to find the actual relationship between the columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fc8386",
   "metadata": {},
   "source": [
    "# Visualization of the  features after treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3010ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['aon'],kde=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7340495",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(x=df['aon'], ec = \"gold\", color='g', kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c72d531",
   "metadata": {},
   "source": [
    "We see a much better distribution in this compared to before , but the majority of th dtat resides on 0 to 0.5 so we cant make a proper prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799a1668",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.subplots(figsize=(12,4))\n",
    "sns.countplot(x=\"aon\", hue=\"label\", data=df, palette=\"colorblind\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf9bc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "plot_df=df.sort_values(by=\"label\")\n",
    "fig=px.histogram(plot_df, x='aon', color='label', \n",
    "                 opacity=0.8, histnorm='density', barmode='overlay', marginal='box',\n",
    "                 color_discrete_map={'Yes': '#B9C0C9','No': '#CDBBA7'})\n",
    "fig.update_layout(title_text='Distribution of aon by label Status',\n",
    "                  xaxis_title='aon, $', yaxis_title='Density',font_color='#28221D',\n",
    "                  paper_bgcolor='#F4F2F0', plot_bgcolor='#F4F2F0', legend_traceorder='reversed')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dc0ec3",
   "metadata": {},
   "source": [
    "As mentioned in the distplot analysis the data is concentrated in the 0 to 0.5 , so we cant see any pattern and we can see that the majority is from class 1 as seen before "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ed08c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['rental30'],kde=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f83be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(x=df['rental30'], ec = \"gold\", color='g', kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1214b3e7",
   "metadata": {},
   "source": [
    "Same as the previous column we see more concentration on 0-0.5 and we see outliers on both ends of the chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f86ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.subplots(figsize=(12,4))\n",
    "sns.countplot(x=\"rental30\", hue=\"label\", data=df, palette=\"colorblind\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a981647e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "plot_df=df.sort_values(by=\"label\")\n",
    "fig=px.histogram(plot_df, x='rental30', color='label', \n",
    "                 opacity=0.8, histnorm='density', barmode='overlay', marginal='box',\n",
    "                 color_discrete_map={'Yes': '#B9C0C9','No': '#CDBBA7'})\n",
    "fig.update_layout(title_text='Distribution of rental30 by label Status',\n",
    "                  xaxis_title='rental30, $', yaxis_title='Density',font_color='#28221D',\n",
    "                  paper_bgcolor='#F4F2F0', plot_bgcolor='#F4F2F0', legend_traceorder='reversed')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c9ac3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b593edb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['rental90'],kde=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db256db",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(x=df['rental90'], ec = \"gold\", color='g', kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c066bcfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7599d52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.subplots(figsize=(12,4))\n",
    "sns.countplot(x=\"rental90\", hue=\"label\", data=df, palette=\"colorblind\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d403fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "plot_df=df.sort_values(by=\"label\")\n",
    "fig=px.histogram(plot_df, x='rental90', color='label', \n",
    "                 opacity=0.8, histnorm='density', barmode='overlay', marginal='box',\n",
    "                 color_discrete_map={'Yes': '#B9C0C9','No': '#CDBBA7'})\n",
    "fig.update_layout(title_text='Distribution of rental90 by label Status',\n",
    "                  xaxis_title='rental90, $', yaxis_title='Density',font_color='#28221D',\n",
    "                  paper_bgcolor='#F4F2F0', plot_bgcolor='#F4F2F0', legend_traceorder='reversed')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5613ddce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2435d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['last_rech_date_ma'],kde=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f3a5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(x=df['last_rech_date_ma'], ec = \"gold\", color='g', kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04be09a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b963945",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.subplots(figsize=(12,4))\n",
    "sns.countplot(x=\"last_rech_date_ma\", hue=\"label\", data=df, palette=\"colorblind\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889bbf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "plot_df=df.sort_values(by=\"label\")\n",
    "fig=px.histogram(plot_df, x='last_rech_date_ma', color='label', \n",
    "                 opacity=0.8, histnorm='density', barmode='overlay', marginal='box',\n",
    "                 color_discrete_map={'Yes': '#B9C0C9','No': '#CDBBA7'})\n",
    "fig.update_layout(title_text='Distribution of last_rech_date_ma by label Status',\n",
    "                  xaxis_title='last_rech_date_ma, $', yaxis_title='Density',font_color='#28221D',\n",
    "                  paper_bgcolor='#F4F2F0', plot_bgcolor='#F4F2F0', legend_traceorder='reversed')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8209723d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa56668a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['last_rech_date_da'],kde=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24686549",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(x=df['last_rech_date_da'], ec = \"gold\", color='g', kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530ca716",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4280ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.subplots(figsize=(12,4))\n",
    "sns.countplot(x=\"last_rech_date_da\", hue=\"label\", data=df, palette=\"colorblind\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82609b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "plot_df=df.sort_values(by=\"label\")\n",
    "fig=px.histogram(plot_df, x='last_rech_date_da', color='label', \n",
    "                 opacity=0.8, histnorm='density', barmode='overlay', marginal='box',\n",
    "                 color_discrete_map={'Yes': '#B9C0C9','No': '#CDBBA7'})\n",
    "fig.update_layout(title_text='Distribution of last_rech_date_da by label Status',\n",
    "                  xaxis_title='last_rech_date_da, $', yaxis_title='Density',font_color='#28221D',\n",
    "                  paper_bgcolor='#F4F2F0', plot_bgcolor='#F4F2F0', legend_traceorder='reversed')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8a2321",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8778ce0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['last_rech_amt_ma'],kde=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f9ea1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(x=df['last_rech_amt_ma'], ec = \"gold\", color='g', kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd24fb95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ab7651",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.subplots(figsize=(12,4))\n",
    "sns.countplot(x=\"last_rech_amt_ma\", hue=\"label\", data=df, palette=\"colorblind\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c325907",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "plot_df=df.sort_values(by=\"label\")\n",
    "fig=px.histogram(plot_df, x='last_rech_amt_ma', color='label', \n",
    "                 opacity=0.8, histnorm='density', barmode='overlay', marginal='box',\n",
    "                 color_discrete_map={'Yes': '#B9C0C9','No': '#CDBBA7'})\n",
    "fig.update_layout(title_text='Distribution of last_rech_amt_ma by label Status',\n",
    "                  xaxis_title='last_rech_amt_ma, $', yaxis_title='Density',font_color='#28221D',\n",
    "                  paper_bgcolor='#F4F2F0', plot_bgcolor='#F4F2F0', legend_traceorder='reversed')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b03d975",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26658fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['fr_ma_rech30'],kde=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6c96be",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(x=df['fr_ma_rech30'], ec = \"gold\", color='g', kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b31d91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b71ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.subplots(figsize=(12,4))\n",
    "sns.countplot(x=\"fr_ma_rech30\", hue=\"label\", data=df, palette=\"colorblind\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316ec891",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "plot_df=df.sort_values(by=\"label\")\n",
    "fig=px.histogram(plot_df, x='fr_ma_rech30', color='label', \n",
    "                 opacity=0.8, histnorm='density', barmode='overlay', marginal='box',\n",
    "                 color_discrete_map={'Yes': '#B9C0C9','No': '#CDBBA7'})\n",
    "fig.update_layout(title_text='Distribution of fr_ma_rech30 by label Status',\n",
    "                  xaxis_title='fr_ma_rech30, $', yaxis_title='Density',font_color='#28221D',\n",
    "                  paper_bgcolor='#F4F2F0', plot_bgcolor='#F4F2F0', legend_traceorder='reversed')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15038322",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23a1eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['sumamnt_ma_rech30'],kde=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5798be",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(x=df['sumamnt_ma_rech30'], ec = \"gold\", color='g', kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c59760",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed41dbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.subplots(figsize=(12,4))\n",
    "sns.countplot(x=\"aon\", hue=\"label\", data=df, palette=\"colorblind\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105cbfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "plot_df=df.sort_values(by=\"label\")\n",
    "fig=px.histogram(plot_df, x='sumamnt_ma_rech30', color='label', \n",
    "                 opacity=0.8, histnorm='density', barmode='overlay', marginal='box',\n",
    "                 color_discrete_map={'Yes': '#B9C0C9','No': '#CDBBA7'})\n",
    "fig.update_layout(title_text='Distribution of sumamnt_ma_rech30 by label Status',\n",
    "                  xaxis_title='sumamnt_ma_rech30, $', yaxis_title='Density',font_color='#28221D',\n",
    "                  paper_bgcolor='#F4F2F0', plot_bgcolor='#F4F2F0', legend_traceorder='reversed')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d00c916",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511e590c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['medianamnt_ma_rech30'],kde=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bfda27",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(x=df['medianamnt_ma_rech30'], ec = \"gold\", color='g', kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c32e94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49aeee7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.subplots(figsize=(12,4))\n",
    "sns.countplot(x=\"medianamnt_ma_rech30\", hue=\"label\", data=df, palette=\"colorblind\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b454b441",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "plot_df=df.sort_values(by=\"label\")\n",
    "fig=px.histogram(plot_df, x='medianamnt_ma_rech30', color='label', \n",
    "                 opacity=0.8, histnorm='density', barmode='overlay', marginal='box',\n",
    "                 color_discrete_map={'Yes': '#B9C0C9','No': '#CDBBA7'})\n",
    "fig.update_layout(title_text='Distribution of medianamnt_ma_rech30 by label Status',\n",
    "                  xaxis_title='medianamnt_ma_rech30, $', yaxis_title='Density',font_color='#28221D',\n",
    "                  paper_bgcolor='#F4F2F0', plot_bgcolor='#F4F2F0', legend_traceorder='reversed')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbf24e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438fb7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['medianmarechprebal30'],kde=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164732fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(x=df['medianmarechprebal30'], ec = \"gold\", color='g', kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1af546",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4009a1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.subplots(figsize=(12,4))\n",
    "sns.countplot(x=\"medianmarechprebal30\", hue=\"label\", data=df, palette=\"colorblind\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b5db1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "plot_df=df.sort_values(by=\"label\")\n",
    "fig=px.histogram(plot_df, x='medianmarechprebal30', color='label', \n",
    "                 opacity=0.8, histnorm='density', barmode='overlay', marginal='box',\n",
    "                 color_discrete_map={'Yes': '#B9C0C9','No': '#CDBBA7'})\n",
    "fig.update_layout(title_text='Distribution of medianmarechprebal30 by label Status',\n",
    "                  xaxis_title='medianmarechprebal30, $', yaxis_title='Density',font_color='#28221D',\n",
    "                  paper_bgcolor='#F4F2F0', plot_bgcolor='#F4F2F0', legend_traceorder='reversed')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa5f677",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c31b59d",
   "metadata": {},
   "source": [
    "# Encoding the categorical Features to numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e33e602",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "enc=LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58b05c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df.columns:\n",
    "    if df[i].dtypes == \"object\":\n",
    "        df[i]=enc.fit_transform(df[i].values.reshape(-1,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a3a376",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2d6b19",
   "metadata": {},
   "source": [
    "We have converted the categorical data to numerical data we can move ahead to the next step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3baa0470",
   "metadata": {},
   "source": [
    "# Visualizing the relationship between the features and the 1st  target variable - Average Cost of two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9379fc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide data into features and label\n",
    "\n",
    "x = df.drop(columns = ['label'])\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab7d509",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81424e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815864be",
   "metadata": {},
   "source": [
    "# Scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9bd1de",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Lets see how the data is distributed for every column as a whole\n",
    "\n",
    "#Visualizing Relationship\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize =(25,30), facecolor = 'yellow')\n",
    "plotnumber = 1\n",
    "\n",
    "for column in x:\n",
    "    if plotnumber <=35:\n",
    "        ax = plt.subplot(9,4,plotnumber)\n",
    "        plt.scatter(x[column],y)\n",
    "        plt.xlabel(column,fontsize = 20)\n",
    "        plt.ylabel('label ',fontsize = 10)\n",
    "    plotnumber +=1\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd3ac06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets see how the data is distributed for every column as a whole\n",
    "\n",
    "#Visualizing Relationship\n",
    "\n",
    "plt.figure(figsize =(25,30), facecolor = 'white')\n",
    "plotnumber = 1\n",
    "\n",
    "for column in x:\n",
    "    if plotnumber <=35:\n",
    "        ax = plt.subplot(9,4,plotnumber)\n",
    "        sns.lineplot(x[column],y)\n",
    "        plt.xlabel(column,fontsize = 20)\n",
    "        plt.ylabel('label',fontsize = 10)\n",
    "    plotnumber +=1\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470a9c4b",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700214d5",
   "metadata": {},
   "source": [
    "### Describing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e21a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b784fb7",
   "metadata": {},
   "source": [
    "# Observations:-\n",
    "\n",
    "    - We see that the aon, daily_decr30, daily_decr90,rental30,rental90,last_rech_date_ma and most of the other columns have extreme values , outliers causing the Std to be higher than the mean whichis not good need to treat them for if  many columns has outliers and the range is too extreme, the min says 0 which is not possible  which is again wrong, we need to treat this  \n",
    "    - we see that the label is classification and series on 0 and 1 \n",
    "details\n",
    "    - the rest 2 are categorical and have ratings "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7291873a",
   "metadata": {},
   "source": [
    "# Visualization of the Data Properties\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9beca5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets see how the data is distributed for every column\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(25,10))\n",
    "sns.heatmap(df.describe(),annot=True,linewidths=0.1,linecolor=\"black\",fmt='0.2f')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f43189",
   "metadata": {},
   "source": [
    "we see the 1st label having skewness and category columns is delivering now as well and votes , we can only treat them using zscore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3b2c01",
   "metadata": {},
   "source": [
    "# Correlation of the columns with the  target variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3b8e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()['label'].sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de7187f",
   "metadata": {},
   "source": [
    "We se that sumamnt_ma_rech90 is having the highest correlation with 36.7% approx amd cnt_ma_rech90 is also having similar at 36.2%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7112d33",
   "metadata": {},
   "source": [
    "## Heatmap of Correlation of the columns within the Columns or Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46656b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#size of canvas\n",
    "plt.figure(figsize=(35,20))\n",
    "sns.heatmap(df.corr(),annot=True, linewidths=0.5,linecolor='black', fmt='.2f')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985839af",
   "metadata": {},
   "source": [
    "From the heat map we see :\n",
    "    . We have many columns which are very correlated to each other , we see that we have 100% relationship between daily_decr30 and daily_decr90 \n",
    "    . we see that rental 30 and rental90 have 96% relationship and amnt_loans30 and cnt_loans30 have 98% relationship with each other \n",
    "    . we see other columns as well haveing 80% or more relationship which can lead to mutiollinearity problem so we will be using feature scaling and other techniques to remove the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4379c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting a barplot to see th relationship with 1st label in a better way\n",
    "\n",
    "df.drop('label', axis=1).corrwith(df['label']).plot(kind='bar', grid=True,figsize=(32,15),\n",
    "                                                  title='Correlation with target')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9524a24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets visualize the data\n",
    "plt.figure(figsize =(20,25))\n",
    "graph = 1\n",
    "\n",
    "for column in df:\n",
    "    if graph <36: # as there are 9 columns in data\n",
    "        plt.subplot(9,4,graph)\n",
    "        ax=sns.boxplot(data=df[column])\n",
    "        plt.xlabel(column,fontsize = 15)\n",
    "        \n",
    "    graph +=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ae1527",
   "metadata": {},
   "source": [
    "We do see some outliers but we have already used power transformer to get rid of the skewness but if we use further removal we have seen that we are removing more than 8% which is allowed so we will move ahead"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7885f2ff",
   "metadata": {},
   "source": [
    "# Using SelectKBest Feature Selection Method - Target \n",
    "\n",
    "Select KBest use f_classif function to find the best features, where f_classif uses Anova Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170d49fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030f9a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again we Divide data into features and label\n",
    "\n",
    "X = df.drop(columns = ['label'])\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574d3b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a9a9c5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_features = SelectKBest(score_func = f_classif, k=35)\n",
    "\n",
    "fit = best_features.fit(X,y)\n",
    "\n",
    "df_scores = pd.DataFrame(fit.scores_)\n",
    "\n",
    "df_columns = pd.DataFrame(X.columns)\n",
    "\n",
    "\n",
    "#concatenate dataframes\n",
    "\n",
    "feature_scores = pd.concat([df_columns, df_scores], axis = 1)\n",
    "\n",
    "feature_scores.columns = ['Feature_name', 'Score']   #name output columns\n",
    "\n",
    "print(feature_scores.nlargest(35,'Score'))  #Print Best features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4205780",
   "metadata": {},
   "source": [
    "### We see that the feature  sumamnt_ma_rech90  32496.962107 cnt_ma_rech90  31711.078501 sumamnt_ma_rech30  30472.719937 cnt_ma_rech30  28251.846485 is the best  as the score they have are greater than 2500 approx which is really high,, the rest of them have a good impact or influence on the  label, but we are only performing this step as a way to analyze the data even further , We see that correlation showed different features and Kbest is showing different so we will move on and we will do some more analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5783681f",
   "metadata": {},
   "source": [
    "# Variance Inflation Factor\n",
    "\n",
    "Checking for Multicollinearity problem to see if one feature is dependent on the other , we need to scale the dat first using MINMAX Scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1883b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "mms=MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdabeee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = mms.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddb0e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bf5eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "vif = pd.DataFrame()\n",
    "vif[\"vif\"] = [variance_inflation_factor(X_scaled, i) for i in range(X_scaled.shape[1])]\n",
    "vif[\"Features\"] = X.columns\n",
    "\n",
    "#chck Values\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3449356",
   "metadata": {},
   "source": [
    "We see a high variance in some of the columns and we need to se if we can remove those features in th next step which is PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507d7397",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA)\n",
    "\n",
    "It is a dimension reduction technique and not a feature selection one.\n",
    "\n",
    "and we are going to apply on the features only , it is mainly used if there are too many features and no correlation with the target\n",
    "\n",
    "but its the final analysis we are going to do to chcek for multicollinearity problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ef8fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a6dde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22a0bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit_transform(X_scaled) #To scale the data with PCA so we can plot the graph to see whats the coverage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ebc25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets plot scree plot to check the best components\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Principal Components')\n",
    "plt.ylabel(\"Variance Covered\")\n",
    "plt.title('PCA')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100f57c9",
   "metadata": {},
   "source": [
    "### We see that in order to cover 95% - 100% of the data we need to have only have 12 features and we can remove the rest , We will use the Kbest to decide which features are the best and see if we should remove any feaures ,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3cc0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "collist=df.columns.values\n",
    "ncol=30\n",
    "nrows=14\n",
    "plt.figure(figsize=(ncol,3*ncol))\n",
    "for i in range (0,len(collist)):\n",
    "    plt.subplot(nrows,ncol,i+1)\n",
    "    sns.boxplot(df_new_z[collist[i]],color='green',orient='h')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61966058",
   "metadata": {},
   "source": [
    "# Creating the Model - Choosing the Best Model- Classification problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792bfa8b",
   "metadata": {},
   "source": [
    "## Conducting PCA again to reduce the features for 1st Label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c97984",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled =scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30677e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit_transform(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d64de18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets plot scree plot to check the bset components\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Principal Components')\n",
    "plt.ylabel(\"Variance Covered\")\n",
    "plt.title('PCA')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855fa484",
   "metadata": {},
   "source": [
    "### Afterdoing PCA treatment we see that we need only 12 features to cover the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e549f7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 12)\n",
    "new_pcomp = pca.fit_transform(X_scaled)\n",
    "princ1_comp = pd.DataFrame(new_pcomp,columns=['PC1','PC2','PC3','PC4','PC5','PC6','PC7','PC8','PC9','PC10','PC11','PC12'])\n",
    "princ1_comp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d86332",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f43061",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr=LogisticRegression()\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, roc_auc_score,classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a7f12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores=[]\n",
    "for i in range(0,100):\n",
    "    X_train,X_test,y_train,y_test = train_test_split(princ1_comp,y,test_size = 0.25,random_state = i)\n",
    "    lr.fit(X_train,y_train)\n",
    "    pred_train = lr.predict(X_train)\n",
    "    pred_test=lr.predict(X_test)\n",
    "    print(f\"At random state {i},the training accuracy is :-{accuracy_score(y_train,pred_train)}\")\n",
    "    print(f\"At random state {i},the Testing accuracy is :-{accuracy_score(y_test,pred_test)}\")\n",
    "    print('\\n')\n",
    "    scores.append(accuracy_score(y_test,pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae843460",
   "metadata": {},
   "source": [
    "Finding the highest score using Argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdc2a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f52ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores[np.argmax(scores)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23260fd",
   "metadata": {},
   "source": [
    "# We see that this model work well with the data , we see that the scores are the same at Training and testing state\n",
    "    \n",
    "    - we are getting \n",
    "     \n",
    "     At random state 67,the training accuracy is :-0.880109927859842\n",
    "     \n",
    "     \n",
    "     At random state 67,the Testing accuracy is :-0.8801862666513989\n",
    "     \n",
    "- the training score and Testing score are equal  to each other here\n",
    "- both the train and test score are really good but we will test more an also th cv score to see if its consistent  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a692bf",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb5508e",
   "metadata": {},
   "source": [
    "### We are creating a method called Metric to allow us to show the metrics of each classification model we use , so we dont have to code it again "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116618a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write one function and call as many times to check accuracy_score of different models\n",
    "\n",
    "def metric_score(clf,X_train,X_test,y_train,y_test,train=True):\n",
    "    if train:\n",
    "        y_pred = clf.predict(X_train)\n",
    "    \n",
    "        \n",
    "        print(\"\\n===============================Train Result=============================\")\n",
    "        \n",
    "        print(f\"Accuracy score : {accuracy_score(y_train,y_pred) * 100: .2f}%\")\n",
    "        \n",
    "    elif train == False:\n",
    "        pred = clf.predict(X_test)\n",
    "        \n",
    "        print(\"\\n===============================Test Result===============================\")\n",
    "        print(f\"Accuracy Scorre : {accuracy_score(y_test,pred) * 100: .2f}%\")\n",
    "        \n",
    "        \n",
    "        print ('\\n \\n Test Classification Report \\n', classification_report(y_test, pred, digits = 2)) ##Model Confidence /Accurancy\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07933d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call the function and pass dataset to check the train score and the test score\n",
    "\n",
    "metric_score(lr,X_train,X_test,y_train,y_test,train=True) #This is for the Training Score\n",
    "\n",
    "metric_score(lr,X_train,X_test,y_train,y_test,train=False) #This is for the Testing Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bb52a9",
   "metadata": {},
   "source": [
    "### We see that this model is having a pretty good score in Logistic regression , we see the train score as 88.01% and the test score as 87.92% which is pretty good considering that both the scores are exactly identical\n",
    "\n",
    "Important to note we have used a different random state as the one we chose was having a higher train score as compared to this one so we changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbbaef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test,pred_test))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff33961",
   "metadata": {},
   "source": [
    "We see that this is multiclass classification so its giving a relatively good score but , and we need to see other models , but before that we will check cv score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c9dd80",
   "metadata": {},
   "source": [
    "# Cross-Validation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfac02fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "for j in range(2,10):\n",
    "    cv_score=cross_val_score(lr,X,y,cv=j)\n",
    "    cv_mean=cv_score.mean()\n",
    "    print(f\"At cross fold{j} the cv score is {cv_mean} and accuracy score for training is {accuracy_score(y_train,pred_train)}and the accuracy for testing is {accuracy_score(y_test,pred_test)}\")\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6361b7",
   "metadata": {},
   "source": [
    "We see that the model is overfitting the data as we see the cv score of 88% approx is giving a test score of 88% approx which is very good but so we need to check other models as this model is not the only one and we have a good test score and its equal to the cv score, Cv score @ 7 is the best "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5baedc0c",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d9bf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt=DecisionTreeClassifier()\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(princ1_comp,y,test_size = 0.25,random_state = 67) #as we have seen a good score on 67th state\n",
    "dt.fit(X_train,y_train)\n",
    "pred_train = dt.predict(X_train)\n",
    "pred_test = dt.predict(X_test)\n",
    "print(f\"At random state {67},the training accuracy is :-{accuracy_score(y_train,pred_train)}\")\n",
    "print(f\"At random state {67},the Testing accuracy is :-{accuracy_score(y_test,pred_test)}\")\n",
    "print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4c85ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call the function and pass dataset to check the train score and the test score\n",
    "\n",
    "metric_score(dt,X_train,X_test,y_train,y_test,train=True) #This is for the Training Score\n",
    "\n",
    "metric_score(dt,X_train,X_test,y_train,y_test,train=False) #This is for the Testing Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d80923",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test,pred_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7548b9",
   "metadata": {},
   "source": [
    "# Observations from the Decision Tree Classifier :-\n",
    "\n",
    "    - We see that the training score is boosted all the way to 100% which is the highest  but the testing score is drastically  better  than logistic regression  @ 84.84 % which is much higher than the logistic model   , also we see that the F1 score is the same as test score for accuracy and precision is 66% ,  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72feeea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross validation of the model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "for j in range(2,10):\n",
    "    cv_score=cross_val_score(dt,princ1_comp,y,cv=j)\n",
    "    cv_mean=cv_score.mean()\n",
    "    print(f\"At cross fold{j} the cv score is {cv_mean} and accuracy score for training is {accuracy_score(y_train,pred_train)}and the accuracy for testing is {accuracy_score(y_test,pred_test)}\")\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb897342",
   "metadata": {},
   "source": [
    "We see a really good improvement with the Cv score compared to the last model , we see the cv score @2 state and test score is coming really close at 85%  which is really good for the model, and the score is really high as well "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7568de",
   "metadata": {},
   "source": [
    "# KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6662ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn=KNeighborsClassifier()\n",
    "\n",
    "knn.fit(X_train,y_train)\n",
    "knn.score(X_train,y_train)\n",
    "pred_decision =knn.predict(X_test)\n",
    "\n",
    "knns = accuracy_score(y_test,pred_decision)\n",
    "print('Accuracy Score :',knns*100)\n",
    "\n",
    "knnscore = cross_val_score(knn,princ1_comp,y,cv=2)\n",
    "knnc =knnscore.mean()\n",
    "print('Cross Val Score :',knnc*100)\n",
    "print(confusion_matrix(y_test,pred_decision)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00739d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call the function and pass dataset to check the train score and the test score\n",
    "\n",
    "metric_score(knn,X_train,X_test,y_train,y_test,train=True) #This is for the Training Score\n",
    "\n",
    "metric_score(knn,X_train,X_test,y_train,y_test,train=False) #This is for the Testing Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2847c68f",
   "metadata": {},
   "source": [
    "# Observations from the KNN Classifier :-\n",
    "    - We see that the training score is lower than Decision tree @ 91.10% but the train score is closer to test @ 88.06%  \n",
    "    \n",
    "    \n",
    "    - the CV score is good though and very similar to the test accuracy @ 87% highest among the 3 models we tested , so overall the model is ok ,\n",
    "    \n",
    "    - we see the confusion matrix where the typ 1 and typ 2 error is much better than the previous models and we see that many of the error are high so its overall a good contender for decision tree model , lets test other models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616e8348",
   "metadata": {},
   "source": [
    "# XgBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a2a192",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xgb = xgb.XGBClassifier()\n",
    "\n",
    "xgb.fit(X_train,y_train)\n",
    "xgb.score(X_train,y_train)\n",
    "pred_decision =xgb.predict(X_test)\n",
    "\n",
    "xgbs = accuracy_score(y_test,pred_decision)\n",
    "print('Accuracy Score :',xgbs*100)\n",
    "\n",
    "xgbscore = cross_val_score(xgb,princ1_comp,y,cv=2)\n",
    "xgbc =xgbscore.mean()\n",
    "print('Cross Val Score :',xgbc*100)\n",
    "print(confusion_matrix(y_test,pred_decision)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0dcf7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call the function and pass dataset to check the train score and the test score\n",
    "\n",
    "metric_score(xgb,X_train,X_test,y_train,y_test,train=True) #This is for the Training Score\n",
    "\n",
    "metric_score(xgb,X_train,X_test,y_train,y_test,train=False) #This is for the Testing Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332863be",
   "metadata": {},
   "source": [
    "# SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccae120",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc= SVC()\n",
    "\n",
    "svc.fit(X_train,y_train)\n",
    "svc.score(X_train,y_train)\n",
    "pred_decision =svc.predict(X_test)\n",
    "\n",
    "svcs = accuracy_score(y_test,pred_decision)\n",
    "print('Accuracy Score :',svcs*100)\n",
    "\n",
    "svcscore = cross_val_score(svc,princ1_comp,y,cv=2)\n",
    "svcc =svcscore.mean()\n",
    "print('Cross Val Score :',svcc*100)\n",
    "print(confusion_matrix(y_test,pred_decision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b8d59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call the function and pass dataset to check the train score and the test score\n",
    "\n",
    "metric_score(svc,X_train,X_test,y_train,y_test,train=True) #This is for the Training Score\n",
    "\n",
    "metric_score(svc,X_train,X_test,y_train,y_test,train=False) #This is for the Testing Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac981a57",
   "metadata": {},
   "source": [
    "# WE have again proved that we will use the XGboost  Classifier as the best  model for the data set so we will move onto ROC AUC before we do hyperparameter tuning to improve the already high score for the best model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c62a984",
   "metadata": {},
   "source": [
    "# Let's check ROC AUC Curve for the fitted Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c60136",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "### How well our model works on training Data\n",
    "\n",
    "disp = plot_roc_curve(lr,X_train,y_train)\n",
    "\n",
    "plot_roc_curve(dt,X_train,y_train, ax= disp.ax_) #ax_ = Axes with confusion matrix\n",
    "\n",
    "plot_roc_curve(knn,X_train,y_train, ax= disp.ax_)\n",
    "\n",
    "plot_roc_curve(xgb,X_train,y_train, ax= disp.ax_)\n",
    "\n",
    "plot_roc_curve(svc,X_train,y_train, ax= disp.ax_)\n",
    "\n",
    "plt.legend(prop={'size' : 10}, loc='lower right' )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e51e728",
   "metadata": {},
   "outputs": [],
   "source": [
    "### How well our model works on Testing Data\n",
    "\n",
    "disp = plot_roc_curve(lr,X_test,y_test)\n",
    "\n",
    "plot_roc_curve(dt,X_test,y_test, ax= disp.ax_) #ax_ = Axes with confusion matrix\n",
    "\n",
    "plot_roc_curve(knn,X_test,y_test, ax= disp.ax_)\n",
    "\n",
    "plot_roc_curve(xgb,X_test,y_test, ax= disp.ax_)\n",
    "\n",
    "plot_roc_curve(svc,X_test,y_test, ax= disp.ax_)\n",
    "\n",
    "plt.legend(prop={'size' : 10}, loc='lower right' )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacad8a6",
   "metadata": {},
   "source": [
    "# We will go with XGB classifier model as :-\n",
    "\n",
    "- the model gives the highest accuracy and other f1 and preciosn scores\n",
    "- the model has lowest error in the confusion matrix\n",
    "- the model is shown that it havs train 94% and 89% in ROC AUC , which is way better that others\n",
    "- we also see that the other models we tesst may have closer scores like LOgistic regression but the score is very low and we cant go with it \n",
    "- Overall we need to improve the cvscore and accuracy in xgboost and if we can do hyperparameter tuning to do it , this will be the best model we can choose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b72f1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b398ae37",
   "metadata": {},
   "source": [
    "# Hyper parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9d7369",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import xgboost as xgb\n",
    "xgb = xgb.XGBClassifier()\n",
    "\n",
    "#Creating parameters to pass in RandomizedSearchCV\n",
    "\n",
    "parameters = {'gamma': [0.1,0.5,1,10,16,32,64],\n",
    "              'learning_rate': [0.01, 0.03, 0.06, 0.1, 0.3, 0.2],\n",
    "              'max_depth': [5,6,7,8,9,10],\n",
    "              'n_estimators': [80,100,120,130,150,200],\n",
    "              'reg_alpha': [0.1,0.5,1,10,16,32,64],\n",
    "              'reg_lambda': [0.1,0.5,1,10,16,32,64]\n",
    "             }\n",
    "\n",
    "RCV = RandomizedSearchCV(xgb,parameters,verbose=2,cv=2, n_jobs = -1, scoring='accuracy')\n",
    "RCV.fit(X_train,y_train) #fitting data into the model\n",
    "RCV.best_params_ #printing the best parameters found by the RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ee41c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xgb = xgb.XGBClassifier(reg_lambda= 1,reg_alpha= 0.5,n_estimators= 80,max_depth= 8,learning_rate= 0.06,gamma= 10)\n",
    "\n",
    "xgb.fit(X_train,y_train)\n",
    "xgb.score(X_train,y_train)\n",
    "pred_decision =xgb.predict(X_test)\n",
    "\n",
    "xgbs = accuracy_score(y_test,pred_decision)\n",
    "print('Accuracy Score :',xgbs*100)\n",
    "\n",
    "xgbscore = cross_val_score(xgb,princ1_comp,y,cv=2)\n",
    "xgbc =xgbscore.mean()\n",
    "print('Cross Val Score :',xgbc*100)\n",
    "print(confusion_matrix(y_test,pred_decision)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bdc035",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting distplot to show equillibrium \n",
    "\n",
    "sns.distplot(y_test-pred_decision)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199b007c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(y_test, pred_decision, alpha = 0.5)\n",
    "plt.xlabel(\"y_test\")\n",
    "plt.ylabel(\"pred_test\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d329882",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'Micro_default.pkl'\n",
    "pickle.dump(xgb,open(filename,'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d889a0",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c616c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = pickle.load(open('Micro_default.pkl','rb'))\n",
    "result = loaded_model.score(X_test,y_test)\n",
    "print(result*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dc79cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "conclusion = pd.DataFrame([loaded_model.predict(X_test)[:],pred_decision[:]],index=['Predicted','Orignal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afeb766b",
   "metadata": {},
   "outputs": [],
   "source": [
    "conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e922019",
   "metadata": {},
   "source": [
    "# We have 52398 columns where the model has predicted and Actuals and the model we have chosen is XG Boost Classifier as the ideal model for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63b0415",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
